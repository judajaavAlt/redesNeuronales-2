{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1e7fdc9",
   "metadata": {},
   "source": [
    "# Modelo RNN Sin Memoria (Simple RNN)\n",
    "\n",
    "En este cuaderno se implementa una Red Neuronal Recurrente (RNN) básica (sin celdas de memoria compleja como LSTM o GRU) para la clasificación de texto.\n",
    "El objetivo es entrenar y evaluar el modelo utilizando los embeddings pre-procesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba5ec8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Configuración del dispositivo (GPU si está disponible)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de7767e",
   "metadata": {},
   "source": [
    "## Carga y Preprocesamiento de Datos\n",
    "\n",
    "Se cargan los archivos `.npy` que contienen los embeddings y las etiquetas.\n",
    "Los datos de entrada tienen la forma `(N, 384)`. Para utilizarlos en una RNN, los redimensionamos a `(N, 1, 384)`, tratando cada embedding como una secuencia de longitud 1.\n",
    "Las etiquetas se ajustan para iniciar en 0 (de 1-5 a 0-4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87c07f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de X_train: torch.Size([1200000, 1, 384])\n",
      "Forma de y_train: torch.Size([1200000])\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos\n",
    "train_X = np.load('train_all.npy')\n",
    "train_y = np.load('train_labels.npy')\n",
    "valid_X = np.load('valid_all.npy')\n",
    "valid_y = np.load('valid_labels.npy')\n",
    "test_X = np.load('test_all.npy')\n",
    "test_y = np.load('test_labels.npy')\n",
    "\n",
    "# Convertir a tensores de PyTorch\n",
    "# Redimensionar X a (Batch, Sequence Length, Features) -> (N, 1, 384)\n",
    "X_train_tensor = torch.tensor(train_X, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "y_train_tensor = torch.tensor(train_y - 1, dtype=torch.long).to(device) # Restar 1 para que las clases sean 0-4\n",
    "\n",
    "X_valid_tensor = torch.tensor(valid_X, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "y_valid_tensor = torch.tensor(valid_y - 1, dtype=torch.long).to(device)\n",
    "\n",
    "X_test_tensor = torch.tensor(test_X, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "y_test_tensor = torch.tensor(test_y - 1, dtype=torch.long).to(device)\n",
    "\n",
    "print(f\"Forma de X_train: {X_train_tensor.shape}\")\n",
    "print(f\"Forma de y_train: {y_train_tensor.shape}\")\n",
    "\n",
    "# Crear DataLoaders\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129ebc33",
   "metadata": {},
   "source": [
    "## Definición del Modelo\n",
    "\n",
    "Se define una arquitectura RNN simple utilizando `nn.RNN` de PyTorch.\n",
    "- **Input Size**: 384 (dimensión del embedding).\n",
    "- **Hidden Size**: 128 (tamaño del estado oculto).\n",
    "- **Output Size**: 5 (número de clases).\n",
    "- **Batch First**: True (para que la entrada sea (Batch, Seq, Feature))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d715918b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNNModel(\n",
      "  (rnn): RNN(384, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimpleRNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, input_size)\n",
    "        # out shape: (batch, seq_len, hidden_size)\n",
    "        # h_n shape: (num_layers, batch, hidden_size)\n",
    "        out, h_n = self.rnn(x)\n",
    "        \n",
    "        # Tomamos la salida del último paso de tiempo\n",
    "        last_out = out[:, -1, :]\n",
    "        \n",
    "        # Capa totalmente conectada para clasificación\n",
    "        out = self.fc(last_out)\n",
    "        return out\n",
    "\n",
    "# Instanciar el modelo\n",
    "input_size = 384\n",
    "hidden_size = 128\n",
    "output_size = 5\n",
    "model = SimpleRNNModel(input_size, hidden_size, output_size).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e7741f",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "\n",
    "Se define la función de pérdida (`CrossEntropyLoss`) y el optimizador (`Adam`).\n",
    "Se entrena el modelo por un número determinado de épocas, monitoreando la pérdida y precisión en el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b1a8dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 1.1183, Train Acc: 50.88%, Valid Loss: 1.0956, Valid Acc: 52.05%\n",
      "Epoch [2/10], Train Loss: 1.0853, Train Acc: 52.28%, Valid Loss: 1.0768, Valid Acc: 52.86%\n",
      "Epoch [2/10], Train Loss: 1.0853, Train Acc: 52.28%, Valid Loss: 1.0768, Valid Acc: 52.86%\n",
      "Epoch [3/10], Train Loss: 1.0735, Train Acc: 52.79%, Valid Loss: 1.0736, Valid Acc: 52.63%\n",
      "Epoch [3/10], Train Loss: 1.0735, Train Acc: 52.79%, Valid Loss: 1.0736, Valid Acc: 52.63%\n",
      "Epoch [4/10], Train Loss: 1.0672, Train Acc: 53.04%, Valid Loss: 1.0683, Valid Acc: 53.44%\n",
      "Epoch [4/10], Train Loss: 1.0672, Train Acc: 53.04%, Valid Loss: 1.0683, Valid Acc: 53.44%\n",
      "Epoch [5/10], Train Loss: 1.0631, Train Acc: 53.26%, Valid Loss: 1.0682, Valid Acc: 53.07%\n",
      "Epoch [5/10], Train Loss: 1.0631, Train Acc: 53.26%, Valid Loss: 1.0682, Valid Acc: 53.07%\n",
      "Epoch [6/10], Train Loss: 1.0601, Train Acc: 53.39%, Valid Loss: 1.0643, Valid Acc: 53.39%\n",
      "Epoch [6/10], Train Loss: 1.0601, Train Acc: 53.39%, Valid Loss: 1.0643, Valid Acc: 53.39%\n",
      "Epoch [7/10], Train Loss: 1.0578, Train Acc: 53.46%, Valid Loss: 1.0623, Valid Acc: 53.40%\n",
      "Epoch [7/10], Train Loss: 1.0578, Train Acc: 53.46%, Valid Loss: 1.0623, Valid Acc: 53.40%\n",
      "Epoch [8/10], Train Loss: 1.0560, Train Acc: 53.58%, Valid Loss: 1.0636, Valid Acc: 53.46%\n",
      "Epoch [8/10], Train Loss: 1.0560, Train Acc: 53.58%, Valid Loss: 1.0636, Valid Acc: 53.46%\n",
      "Epoch [9/10], Train Loss: 1.0544, Train Acc: 53.61%, Valid Loss: 1.0608, Valid Acc: 53.33%\n",
      "Epoch [9/10], Train Loss: 1.0544, Train Acc: 53.61%, Valid Loss: 1.0608, Valid Acc: 53.33%\n",
      "Epoch [10/10], Train Loss: 1.0531, Train Acc: 53.70%, Valid Loss: 1.0636, Valid Acc: 53.54%\n",
      "Epoch [10/10], Train Loss: 1.0531, Train Acc: 53.70%, Valid Loss: 1.0636, Valid Acc: 53.54%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += y_batch.size(0)\n",
    "        correct_train += (predicted == y_batch).sum().item()\n",
    "        \n",
    "    train_acc = 100 * correct_train / total_train\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    # Validación\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    correct_valid = 0\n",
    "    total_valid = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in valid_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            valid_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_valid += y_batch.size(0)\n",
    "            correct_valid += (predicted == y_batch).sum().item()\n",
    "            \n",
    "    valid_acc = 100 * correct_valid / total_valid\n",
    "    avg_valid_loss = valid_loss / len(valid_loader)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "          f'Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
    "          f'Valid Loss: {avg_valid_loss:.4f}, Valid Acc: {valid_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f30f2d",
   "metadata": {},
   "source": [
    "## Evaluación\n",
    "\n",
    "Se evalúa el modelo con el conjunto de prueba (`test_all.npy`).\n",
    "Se calcula la precisión final y se muestra un reporte de clasificación detallado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b54aac4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 53.42%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Clase 1       0.67      0.62      0.65      6000\n",
      "     Clase 2       0.45      0.48      0.46      6000\n",
      "     Clase 3       0.42      0.45      0.43      6000\n",
      "     Clase 4       0.48      0.40      0.44      6000\n",
      "     Clase 5       0.65      0.72      0.69      6000\n",
      "\n",
      "    accuracy                           0.53     30000\n",
      "   macro avg       0.54      0.53      0.53     30000\n",
      "weighted avg       0.54      0.53      0.53     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=['Clase 1', 'Clase 2', 'Clase 3', 'Clase 4', 'Clase 5']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
